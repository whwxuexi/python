print('%f %f \n'%( theta[0], theta[1]))
plt.plot(X[:,1],np.dot(X,theta),'-')
plt.legend(['Linear regression','Training data'])
plt.show()
 
predict1=np.dot(np.array([1,3.5]),theta)
predict2=np.dot(np.array([1,7]),theta)
print('For population = 35000, we predict a profit of %f\n'%(predict1[0]*10000))
#不输出[0]的话，会将原数组输出10000遍的
print('For population = 70,000, we predict a profit of %f\n'%(predict2[0]*10000))
 
##########Part3: Visualizing J(theta_0, theta_1)
print('Visualizing J(theta_0, theta_1) ...\n')
 
theta0_vals = np.linspace(-10, 10, 100)
theta11_vals = np.linspace(-1, 4, 100)
 
J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))
 
for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t=np.array([theta0_vals[i],theta1_vals[j]]).reshape((2,1))
        J_vals[i,j]=computeCost(X,y,t)
 
J_vals=J_vals.T
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = Axes3D(fig)
ax.plot_surface(theta0_vals,theta1_vals,J_vals,rstride=1, cstride=1, cmap='rainbow')
plt.xlabel('theta_0')
plt.ylabel('theta_1')
 
#画出等高线
plt.figure()
#填充颜色，20是等高线分为几部分
plt.contourf(theta0_vals, theta1_vals, J_vals, 20, alpha = 0.6, cmap = plt.cm.hot)
plt.contour(theta0_vals, theta1_vals, J_vals, colors = 'black')
plt.plot(theta[0], theta[1], 'r',marker='x', markerSize=10, LineWidth=2)#画点
plt.show()

